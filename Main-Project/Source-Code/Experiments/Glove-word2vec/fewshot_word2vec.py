# -*- coding: utf-8 -*-
"""FewShot_Word2Vec.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NsECEy9rwJbfit666Yx_tdVcIPJi9LEw
"""

import pandas as pd
import numpy as np
import re
from random import seed
from random import sample

seed(42)
np.random.seed(42)

from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

import gensim.downloader as api
from gensim.models.keyedvectors import Word2VecKeyedVectors

from sklearn.decomposition import PCA
from sklearn.metrics import accuracy_score
from scipy import spatial

from nltk.corpus import stopwords

#model = api.load('glove-twitter-25') Not used
model2 = api.load('word2vec-google-news-300')

df = pd.read_csv("latest_ticket_data.csv")
df.head()

def get_only_chars(line):

    clean_line = ""

    line = line.replace("â€™", "")
    line = line.replace("'", "")
    line = line.replace("-", " ") #replace hyphens with spaces
    line = line.replace("\t", " ")
    line = line.replace("\n", " ")
    line = line.lower()

    for char in line:
        if char in 'qwertyuiopasdfghjklzxcvbnm ':
            clean_line += char
        else:
            clean_line += ' '

    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces
    if clean_line[0] == ' ':
        clean_line = clean_line[1:]
    return clean_line

df['Text'] = df['Description'].apply(lambda x: get_only_chars(x))

num_classes = 5
sample_size = 100

from sklearn.preprocessing import LabelEncoder

LE = LabelEncoder()
df['Label'] = LE.fit_transform(df['Category'])

print(df)

# Generate samples that contains K samples of each class

def gen_sample(sample_size, num_classes):

    df_1 = df[(df["Label"]<num_classes + 1)].reset_index().drop(["index"], axis=1).reset_index().drop(["index"], axis=1)
    train = df_1[df_1["Label"] == np.unique(df_1['Label'])[0]].sample(sample_size)

    train_index = train.index.tolist()

    for i in range(1,num_classes):
        train_2 = df_1[df_1["Label"] == np.unique(df_1['Label'])[i]].sample(sample_size)
        train = pd.concat([train, train_2], axis=0)
        train_index.extend(train_2.index.tolist())

    test = df_1[~df_1.index.isin(train_index)]

    return train, test

train, test = gen_sample(sample_size, num_classes)

X_train = train['Text']
y_train = train['Category'].values
X_test = test['Text']
y_test = test['Category'].values

test.shape

train

# Text processing (split, find token id, get embedidng)
def transform_sentence(text, model):

    """
    Mean embedding vector
    """

    def preprocess_text(raw_text, model=model):

        """
        Excluding unknown words and get corresponding token
        """

        raw_text = raw_text.split()

        return list(filter(lambda x: x in model.key_to_index , raw_text))

    tokens = preprocess_text(text)

    if not tokens:
        return np.zeros(model.vector_size)

    text_vector = np.mean(model[tokens], axis=0)

    return np.array(text_vector)

X_train_mean = X_train.apply(lambda x : transform_sentence(x, model2))
X_test_mean = X_test.apply(lambda x : transform_sentence(x, model2))

X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

# Use cosine similarity to find closest class
def classify_txt(txt, mean_embedding):

    best_dist = 1
    best_label = -1

    for cl in range(num_classes):

        dist = spatial.distance.cosine(transform_sentence(txt, model2), mean_embedding[cl])

        if dist < best_dist :
            best_dist = dist
            best_label = cl+1

    return best_label

# Process text and predict on the test set
def return_score(sample_size, num_classes):

    train, test = gen_sample(sample_size, num_classes)

    X_train = train['Text']
    y_train = train['Label'].values
    X_test = test['Text']
    y_test = test['Label'].values

    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model2))
    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model2))

    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

    mean_embedding = {}
    for cl in range(num_classes):
        mean_embedding[cl] = np.mean((X_train_mean[y_train == cl + 1]), axis=0)

    y_pred = [classify_txt(t, mean_embedding) for t in test['Text'].values]

    return accuracy_score(y_pred, y_test)

all_accuracy = {2:[],3:[],4:[],5:[]}

for num_samples in range(1,50):
    for num_cl in range(2, 5):
    #num_cl=5
      all_accuracy[num_cl].append(return_score(num_samples,num_cl))

plt.figure(figsize=(12,8))
plt.plot(all_accuracy[2], label="2 classes")
plt.plot(all_accuracy[3], label="3 classes")
plt.plot(all_accuracy[4], label="4 classes")
plt.plot(all_accuracy[5], label="5 classes")
plt.title("Accuracy depending on the number of samples and classes")
plt.legend()
plt.show()

from sklearn.neighbors import KNeighborsClassifier

def return_score(sample_size, num_classes):

    train, test = gen_sample(sample_size, num_classes)

    X_train = train['Text']
    y_train = train['Label'].values
    X_test = test['Text']
    y_test = test['Label'].values

    X_train_mean = X_train.apply(lambda x : transform_sentence(x, model2))
    X_test_mean = X_test.apply(lambda x : transform_sentence(x, model2))

    X_train_mean = pd.DataFrame(X_train_mean)['Text'].apply(pd.Series)
    X_test_mean = pd.DataFrame(X_test_mean)['Text'].apply(pd.Series)

    clf = KNeighborsClassifier(n_neighbors=sample_size, p=2)
    clf.fit(X_train_mean, y_train)

    y_pred = clf.predict(X_test_mean)

    return accuracy_score(y_pred, y_test)

all_accuracy_knn = {2:[],3:[],4:[],5:[]}

for num_samples in range(1,50):

    for num_cl in range(2, 5):

        all_accuracy_knn[num_cl].append(return_score(num_samples,num_cl))

plt.figure(figsize=(12,8))
plt.plot(all_accuracy_knn[2], label="2 classes")
plt.plot(all_accuracy_knn[3], label="3 classes")
plt.plot(all_accuracy_knn[4], label="4 classes")
plt.plot(all_accuracy_knn[5], label="5 classes")
plt.title("Accuracy depending on the number of samples and classes")
plt.legend()
plt.show()

df_results = pd.DataFrame({
    'Nb Classes':[2,3,4],
    'min K-NN':[min(all_accuracy_knn[2]),
        min(all_accuracy_knn[3]),
        min(all_accuracy_knn[4])],
    'min Cosine':[min(all_accuracy[2]),
        min(all_accuracy[3]),
        min(all_accuracy[4])],
    'mean K-NN':[np.mean(all_accuracy_knn[2]),
        np.mean(all_accuracy_knn[3]),
        np.mean(all_accuracy_knn[4])],
    'mean Cosine':[np.mean(all_accuracy[2]),
        np.mean(all_accuracy[3]),
        np.mean(all_accuracy[4])],
    'max K-NN':[max(all_accuracy_knn[2]),
        max(all_accuracy_knn[3]),
        max(all_accuracy_knn[4])],
    'max Cosine':[max(all_accuracy[2]),
        max(all_accuracy[3]),
        max(all_accuracy[4])]
    })

df_results

import pickle

# save the model to disk
filename = 'finalword2vec-google-news-300.sav'
pickle.dump(model2, open(filename, 'wb'))