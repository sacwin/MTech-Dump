# -*- coding: utf-8 -*-
"""All_tickets_Gensim.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TMEthPlCRM8uhO3EBrrF-O6mLTINFFlM
"""

!pip install nltk

import nltk
import os
import pandas as pd
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize
import numpy as np
from gensim.models import Nmf
from gensim.models.ldamodel import LdaModel
from gensim.models import LsiModel
from gensim.models import HdpModel
from gensim.models import LdaMulticore
from gensim.models import EnsembleLda
from gensim.models.ldamulticore import LdaMulticore
from gensim.models.coherencemodel import CoherenceModel
import gensim
import gensim.corpora as corpora
from gensim.models import Phrases
from gensim.utils import simple_preprocess
from gensim.corpora.dictionary import Dictionary
from gensim.utils import tokenize
from nltk.corpus import stopwords
from collections import Counter
nltk.download('stopwords')
nltk.download('wordnet')
import re
import json
import string
from bs4 import BeautifulSoup
from gensim.parsing.preprocessing import (
    strip_non_alphanum,
    split_alphanum,
    strip_short,
    strip_numeric
)
from collections import Counter
from wordcloud import WordCloud, STOPWORDS
from PIL import Image
import seaborn as sns
import matplotlib.pyplot as plt

datafile = 'all_tickets.csv'
data = pd.read_csv(datafile)

data.head(5)

stop_words = stopwords.words('english')
custom_stop_words = ['hi', 'since', 'please', 'best', 'regards', 'thank', 'thanks', 'hello', 'sent', 'great', 'dear', 'help', 'kind']
time_words = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'today' , 'yesterday', 'tomorrow', 'hour', 'hours', 'time', 'times', 'timelines', 'date', 'day', 'days', 'am', 'pm', 'morning', 'noon', 'afternoon', 'evening', 'night', 'winter', 'summer', 'rain', 'cold']

def remove_stop_words(text):
    pattern = re.compile(r'\b(' + r'|'.join(stop_words) + r')\b\s*')
    text = pattern.sub('', text)
    return text

def remove_custom_words(text):
    pattern = re.compile(r'\b(' + r'|'.join(custom_stop_words) + r')\b\s*')
    text = pattern.sub('', text)
    return text

def remove_time_words(text):
    pattern = re.compile(r'\b(' + r'|'.join(time_words) + r')\b\s*')
    text = pattern.sub('', text)
    return text

data['body'] = data["body"].map(lambda x: remove_stop_words(x))
data['Description'] = data["body"].map(lambda x: remove_custom_words(x))
data['body'] = data["body"].map(lambda x: remove_time_words(x))

def parse_html(text, parser="html.parser"):
    soup = BeautifulSoup(text, parser)
    soup = remove_html_tags(soup)
    text = remove_multiple_space(soup.get_text()).strip()
    return text


def parse_html_v2(text, loop=2, parser="html.parser"):
    if not text:
        text = ""
    # some contents still have html code after first parse
    # loop solved problem
    for _ in range(loop):
        soup = BeautifulSoup(text, parser)
        text = soup.get_text()
    text = remove_multiple_space(text)
    return text


def remove_links_content(text):
    text = re.sub(r"http\S+", "", text)
    return text


def remove_emails(text):
    return re.sub('\S*@\S*\s?', '', text)  # noqa


def remove_punctuation(text):
    """https://stackoverflow.com/a/37221663"""
    table = str.maketrans({key: None for key in string.punctuation})
    return text.translate(table)


def remove_special_tags(text):
    """Remove html tags from a string"""
    clean = re.compile('{.*?}')
    return re.sub(clean, '', text)


def preprocess_text(text):
    text = parse_html_v2(text)
    text = text.lower()
    text = remove_links_content(text)
    text = remove_emails(text)
    text = remove_special_tags(text)  # remove content between {}
    text = remove_punctuation(text)  # remove all puntuations
    text = split_alphanum(text)  # add space between word and numeric
    text = strip_numeric(text)  # remove digits
    text = strip_non_alphanum(text)  # remove non-alphabetic characters
    text = strip_short(text, minsize=2)  # remove word with length  <  minsize
    text = remove_multiple_space(text).strip()  # remove space and strip
    #text = tokenize(text)
    return text


def remove_multiple_space(text):
    return re.sub("\s\s+", " ", text)  # noqa


def remove_html_tags(soup,
                     tags=["script", "style"],
                     get_text=False):
    for tag in tags:
        for sample in soup.find_all(tag):
            sample.replaceWith('')

    if get_text:
        return soup.get_text()
    return soup

# Convert body column to string for performing text operations
data['body'] = data['body'].astype(str)
data['body'] =data["body"].map(lambda x: preprocess_text(x))

lemmatizer = WordNetLemmatizer()
data['Lemma_body'] = data["body"].map(lambda x: lemmatizer.lemmatize(x))

def sent_to_words(sentences):
    for sentence in sentences:
        # deacc=True removes punctuations
        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))


x = data['Lemma_body'].values.tolist()
docs = list(sent_to_words(x))
bigram = Phrases(docs, min_count=10)
trigram = Phrases(bigram[docs])

for idx in range(len(docs)):
    for token in bigram[docs[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            docs[idx].append(token)
    for token in trigram[docs[idx]]:
        if '_' in token:
            # Token is a bigram, add to document.
            docs[idx].append(token)

#Remove rare & common tokens
# Create a dictionary representation of the documents.
dictionary = Dictionary(docs)
dictionary.filter_extremes(no_below=10, no_above=0.2)
#Create dictionary and corpus required for Topic Modeling
corpus = [dictionary.doc2bow(doc) for doc in docs]
print('Number of unique tokens: %d' % len(dictionary))
print('Number of documents: %d' % len(corpus))
print(corpus[:1])

# Set parameters.
num_topics = 5
chunksize = 500
passes = 20
iterations = 400
eval_every = 1

# Make a index to word dictionary.
temp = dictionary[0]  # only to "load" the dictionary.
id2word = dictionary

from sklearn.preprocessing import LabelEncoder
LE = LabelEncoder()
data['Label'] = LE.fit_transform(data['category'])

from sklearn.metrics import accuracy_score
def getLDAModelAccuracy(model, corpus, dataset, Label):
  get_document_topics = [model.get_document_topics(item) for item in corpus]
  np_array_of_objects = np.asarray(get_document_topics, dtype="object")
  pred_df = pd.DataFrame(columns=['pred_label'])
  length = len(pd.DataFrame(np_array_of_objects)[0])
  dict_list = []
  for i in range(0,length):
    count=len(pd.DataFrame(np_array_of_objects)[0][i])
    max=0
    for j in range(0,count):
      if(max < (pd.DataFrame(np_array_of_objects)[0][i])[j][1]):
        max=(pd.DataFrame(np_array_of_objects)[0][i])[j][1]
        label=(pd.DataFrame(np_array_of_objects)[0][i])[j][0]
    row_dict = {'pred_label': label}
    dict_list.append(row_dict)
  pred_df = pd.DataFrame.from_dict(dict_list)
  y_pred = pred_df['pred_label'].values
  y_true = dataset['Label'].values
  return(accuracy_score(y_true, y_pred))

from sklearn.metrics import accuracy_score
def getModelAccuracy(model, corpus, dataset, Label):
  get_document_topics=model.__getitem__(corpus)
  #np_array_of_objects = np.asarray(get_document_topics, dtype="object")
  pred_df = pd.DataFrame(columns=['pred_label'])
  length = len(get_document_topics)
  dict_list = []
  for i in range(0,length):
    count=len(get_document_topics[i])
    maxvalue=0
    maxlabel=""
    for j in range(0,count):
      if(maxvalue < get_document_topics[i][j][1]):
        maxvalue=get_document_topics[i][j][1]
        maxlabel=get_document_topics[i][j][0]
    row_dict = {'pred_label': maxlabel}
    dict_list.append(row_dict)
  pred_df = pd.DataFrame.from_dict(dict_list)
  y_pred = pred_df['pred_label'].values
  y_true = dataset['Label'].values
  return(accuracy_score(y_true, y_pred))

#nmf model
nmf_model = Nmf(corpus=corpus, id2word=id2word, chunksize=chunksize, \
                       w_max_iter=iterations, num_topics=num_topics, \
                       passes=passes, eval_every=eval_every)
# Print the Keyword in the 10 topics
print(nmf_model.print_topics())

c_v_coherence_model_nmf = CoherenceModel(model=nmf_model, texts=docs, dictionary=dictionary, coherence='c_v')
c_v_coherence_nmf = c_v_coherence_model_nmf.get_coherence()
print('\nC_V Coherence Score: ', c_v_coherence_nmf)

u_mass_coherence_model_nmf = CoherenceModel(model=nmf_model, texts=docs, dictionary=dictionary, coherence='u_mass')
u_mass_coherence_nmf = u_mass_coherence_model_nmf.get_coherence()
print('\nU_Mass Coherence Score: ', u_mass_coherence_nmf)

c_uci_coherence_model_nmf = CoherenceModel(model=nmf_model, texts=docs, dictionary=dictionary, coherence='c_uci')
c_uci_coherence_nmf = c_uci_coherence_model_nmf.get_coherence()
print('\nc_uci Coherence Score: ', c_uci_coherence_nmf)

c_npmi_coherence_model_nmf = CoherenceModel(model=nmf_model, texts=docs, dictionary=dictionary, coherence='c_npmi')
c_npmi_coherence_nmf = c_npmi_coherence_model_nmf.get_coherence()
print('\c_npmi_Coherence Score: ', c_npmi_coherence_nmf)

#lda model
lda_model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \
                       alpha='auto', eta='auto', \
                       iterations=iterations, num_topics=num_topics, \
                       passes=passes, eval_every=eval_every)
# Print the Keyword in the 10 topics
print(lda_model.print_topics())

getLDAModelAccuracy(lda_model,corpus,data,"Label")

c_v_coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')
c_v_coherence_lda = c_v_coherence_model_lda.get_coherence()
print('\nC_V Coherence Score: ', c_v_coherence_lda)

u_mass_coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='u_mass')
u_mass_coherence_lda = u_mass_coherence_model_lda.get_coherence()
print('\nU_Mass Coherence Score: ', u_mass_coherence_lda)

c_uci_coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_uci')
c_uci_coherence_lda = c_uci_coherence_model_lda.get_coherence()
print('\nc_uciCoherence Score: ', c_uci_coherence_lda)

c_npmi_coherence_model_lda = CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_npmi')
c_npmi_coherence_lda = c_npmi_coherence_model_lda.get_coherence()
print('\c_npmi_Coherence Score: ', c_npmi_coherence_lda)

lsi_model = LsiModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \
                       num_topics=num_topics)
# Print the Keyword in the 10 topics
print(lsi_model.print_topics())

c_v_coherence_model_lsi = CoherenceModel(model=lsi_model, texts=docs, dictionary=dictionary, coherence='c_v')
c_v_coherence_lsi = c_v_coherence_model_lsi.get_coherence()
print('\nC_V Coherence Score: ', c_v_coherence_lsi)

u_mass_coherence_model_lsi = CoherenceModel(model=lsi_model, texts=docs, dictionary=dictionary, coherence='u_mass')
u_mass_coherence_lsi = u_mass_coherence_model_lsi.get_coherence()
print('\nu_mass Coherence Score: ', u_mass_coherence_lsi)

c_uci_coherence_model_lsi = CoherenceModel(model=lsi_model, texts=docs, dictionary=dictionary, coherence='c_uci')
c_uci_coherence_lsi = c_uci_coherence_model_lsi.get_coherence()
print('\nc_uci Coherence Score: ', c_uci_coherence_lsi)

c_npmi_coherence_model_lsi = CoherenceModel(model=lsi_model, texts=docs, dictionary=dictionary, coherence='c_npmi')
c_npmi_coherence_lsi = c_npmi_coherence_model_lsi.get_coherence()
print('\nc_npmi Coherence Score: ', c_npmi_coherence_lsi)

#hdp model
hdp_model = HdpModel(corpus=corpus, id2word=id2word, chunksize=chunksize)
# Print the Keyword in the 10 topics
print(hdp_model.print_topics())

c_v_coherence_model_hdp = CoherenceModel(model=hdp_model, texts=docs, dictionary=dictionary, coherence='c_v')
c_v_coherence_hdp = c_v_coherence_model_hdp.get_coherence()
print('\nC_V Coherence Score: ', c_v_coherence_hdp)

u_mass_coherence_model_hdp = CoherenceModel(model=hdp_model, texts=docs, dictionary=dictionary, coherence='u_mass')
u_mass_coherence_hdp = u_mass_coherence_model_hdp.get_coherence()
print('\nu_mass Coherence Score: ', u_mass_coherence_hdp)

c_uci_coherence_model_hdp = CoherenceModel(model=hdp_model, texts=docs, dictionary=dictionary, coherence='c_uci')
c_uci_coherence_hdp = c_uci_coherence_model_hdp.get_coherence()
print('\nc_uci Coherence Score: ', c_uci_coherence_hdp)

c_npmi_coherence_model_hdp = CoherenceModel(model=hdp_model, texts=docs, dictionary=dictionary, coherence='c_npmi')
c_npmi_coherence_hdp = c_npmi_coherence_model_hdp.get_coherence()

print('\nc_npmi Coherence Score: ', c_npmi_coherence_hdp)

#lda multicore model
lda_multicore_model = LdaMulticore(corpus=corpus, id2word=id2word, chunksize=chunksize, \
                       eta='auto', \
                       iterations=iterations, num_topics=num_topics, \
                       passes=passes, eval_every=eval_every)
# Print the Keyword in the 10 topics
print(lda_multicore_model.print_topics())

getLDAModelAccuracy(lda_multicore_model,corpus,data,"Label")

for t in range(lda_multicore_model.num_topics):
    plt.figure()
    plt.imshow(WordCloud().fit_words(dict(lda_multicore_model.show_topic(t, 200))))
    plt.axis("off")
    plt.title("Topic #" + str(t))
    plt.show()

c_v_coherence_model_lda_multicore = CoherenceModel(model=lda_multicore_model, texts=docs, dictionary=dictionary, coherence='c_v')
c_v_coherence_lda_multicore = c_v_coherence_model_lda_multicore.get_coherence()
print('\nC_V Coherence Score: ', c_v_coherence_lda_multicore)

u_mass_coherence_model_lda_multicore = CoherenceModel(model=lda_multicore_model, texts=docs, dictionary=dictionary, coherence='u_mass')
u_mass_coherence_lda_multicore = u_mass_coherence_model_lda_multicore.get_coherence()
print('\nu_mass Coherence Score: ', u_mass_coherence_lda_multicore)

c_uci_coherence_model_lda_multicore = CoherenceModel(model=lda_multicore_model, texts=docs, dictionary=dictionary, coherence='c_uci')
c_uci_coherence_lda_multicore = c_uci_coherence_model_lda_multicore.get_coherence()
print('\nc_uci Coherence Score: ', c_uci_coherence_lda_multicore)

c_npmi_coherence_model_lda_multicore = CoherenceModel(model=lda_multicore_model, texts=docs, dictionary=dictionary, coherence='c_npmi')
c_npmi_coherence_lda_multicore = c_npmi_coherence_model_lda_multicore.get_coherence()
print('\nc_npmi Coherence Score: ', c_npmi_coherence_lda_multicore)

#ensemble lda
ensemble_lda_model = EnsembleLda(corpus=corpus, id2word=id2word, chunksize=chunksize, \
                       eta='auto', \
                       iterations=iterations, num_topics=num_topics, \
                       passes=passes, eval_every=eval_every)

c_v_coherence_model_ensemble_lda = CoherenceModel(model=ensemble_lda_model, texts=docs, dictionary=dictionary, coherence='c_v')
c_v_coherence_ensemble_lda = c_v_coherence_model_ensemble_lda.get_coherence()
print('\nC_V Coherence Score: ', c_v_coherence_ensemble_lda)

u_mass_coherence_model_ensemble_lda = CoherenceModel(model=ensemble_lda_model, texts=docs, dictionary=dictionary, coherence='u_mass')
u_mass_coherence_ensemble_lda = u_mass_coherence_model_ensemble_lda.get_coherence()
print('\nu_mass Coherence Score: ', u_mass_coherence_ensemble_lda)

c_uci_coherence_model_ensemble_lda = CoherenceModel(model=ensemble_lda_model, texts=docs, dictionary=dictionary, coherence='c_uci')
c_uci_coherence_ensemble_lda = c_uci_coherence_model_ensemble_lda.get_coherence()
print('\nc_uci Coherence Score: ', c_uci_coherence_ensemble_lda)

c_npmi_coherence_model_ensemble_lda = CoherenceModel(model=ensemble_lda_model, texts=docs, dictionary=dictionary, coherence='c_npmi')
c_npmi_coherence_ensemble_lda = c_npmi_coherence_model_ensemble_lda.get_coherence()
print('\nc_npmi Coherence Score: ', c_npmi_coherence_ensemble_lda)