# -*- coding: utf-8 -*-
"""WSTM_TFIDF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/163mH-oFYWWAJRQ7KZjFUCmRRDArbe6_a
"""

# Import Libraries
import pandas as pd
import io
import numpy as np
import re

import nltk
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
from nltk.corpus import wordnet
from nltk.corpus import stopwords

from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score

df = pd.read_csv('latest_ticket_data.csv')

def get_only_chars(line):

    clean_line = ""

    line = line.replace("â€™", "")
    line = line.replace("'", "")
    line = line.replace("-", " ") #replace hyphens with spaces
    line = line.replace("\t", " ")
    line = line.replace("\n", " ")
    line = line.lower()

    for char in line:
        if char in 'qwertyuiopasdfghjklzxcvbnm ':
            clean_line += char
        else:
            clean_line += ' '

    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces
    if clean_line[0] == ' ':
        clean_line = clean_line[1:]
    return clean_line

df['Description'] = df['Description'].apply(lambda x: get_only_chars(x))

df['Tokenized']=[nltk.word_tokenize(i) for i in df['Description']]

df['pos_tagged'] = [nltk.pos_tag(i) for i in df['Tokenized']]

df['NN_tagged'] = df['pos_tagged'].apply(lambda item:[w for w,t in item if t=='NN'])

df['NN_Description'] = df.NN_tagged.map(lambda x: ' '.join(x))

stop_words = stopwords.words('english')
custom_stop_words = ['hi', 'since', 'please', 'best', 'regards', 'thank', 'thanks', 'hello', 'sent', 'great', 'dear', 'help', 'kind']
time_words = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december',
              'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'today' , 'yesterday', 'tomorrow',
              'hour', 'hours', 'time', 'times', 'timelines', 'date', 'day', 'days', 'am', 'pm', 'morning', 'noon', 'afternoon', 'evening',
              'night', 'winter', 'summer', 'rain', 'cold']

def remove_stop_words(text):
    pattern = re.compile(r'\b(' + r'|'.join(stop_words) + r')\b\s*')
    text = pattern.sub('', text)
    return text

def remove_custom_words(text):
    pattern = re.compile(r'\b(' + r'|'.join(custom_stop_words) + r')\b\s*')
    text = pattern.sub('', text)
    return text

def remove_time_words(text):
    pattern = re.compile(r'\b(' + r'|'.join(time_words) + r')\b\s*')
    text = pattern.sub('', text)
    return text

df['NN_Description'] = df["NN_Description"].map(lambda x: remove_stop_words(x))
df['NN_Description'] = df["NN_Description"].map(lambda x: remove_custom_words(x))
df['NN_Description'] = df["NN_Description"].map(lambda x: remove_time_words(x))

LE = LabelEncoder()
df['Label'] = LE.fit_transform(df['Category'])
print(df)

df_0 = df.loc[(df['Label'] == 0)]
df_1 = df.loc[(df['Label'] == 1)]
df_2 = df.loc[(df['Label'] == 2)]
df_3 = df.loc[(df['Label'] == 3)]
df_4 = df.loc[(df['Label'] == 4)]

# Instantiate
vectorizer = TfidfVectorizer()
# Fit the data
tfidf_0 = vectorizer.fit_transform(df_0['NN_Description'])
# Create a dataframe of TFIDF
tfidf_df_0 = pd.DataFrame(tfidf_0[0].T.todense(),
                      index=vectorizer.get_feature_names_out(),
                      columns=["TF-IDF"])
# Sort
tfidf_df_0 = tfidf_df_0.sort_values('TF-IDF', ascending=False)
tfidf_df_0[:20].plot.bar(title="Top 30 TF-IDF Words")

tf_keyword_0 = ['recruiter','permission', 'lead']
tf_keyword_0

# Instantiate
vectorizer = TfidfVectorizer()
# Fit the data
tfidf_1 = vectorizer.fit_transform(df_1['NN_Description'])
# Create a dataframe of TFIDF
tfidf_df_1 = pd.DataFrame(tfidf_1[0].T.todense(),
                      index=vectorizer.get_feature_names_out(),
                      columns=["TF-IDF"])
# Sort
tfidf_df_1 = tfidf_df_1.sort_values('TF-IDF', ascending=False)
tfidf_df_1[:20].plot.bar(title="Top 30 TF-IDF Words")

tf_keyword_1 = ['item','number','user','subscription','name']
tf_keyword_1

# Instantiate
vectorizer = TfidfVectorizer()
# Fit the data
tfidf_2 = vectorizer.fit_transform(df_2['NN_Description'])
# Create a dataframe of TFIDF
tfidf_df_2 = pd.DataFrame(tfidf_2[0].T.todense(),
                      index=vectorizer.get_feature_names_out(),
                      columns=["TF-IDF"])
# Sort
tfidf_df_2 = tfidf_df_2.sort_values('TF-IDF', ascending=False)
tfidf_df_2[:20].plot.bar(title="Top 30 TF-IDF Words")

tf_keyword_2 = ['test','scenario','mobile','device','order']
tf_keyword_2

# Instantiate
vectorizer = TfidfVectorizer()
# Fit the data
tfidf_3 = vectorizer.fit_transform(df_3['NN_Description'])
# Create a dataframe of TFIDF
tfidf_df_3 = pd.DataFrame(tfidf_3[0].T.todense(),
                      index=vectorizer.get_feature_names_out(),
                      columns=["TF-IDF"])
# Sort
tfidf_df_3 = tfidf_df_3.sort_values('TF-IDF', ascending=False)
# Bar Plot
tfidf_df_3[:20].plot.bar(title="Top 30 TF-IDF Words")

tf_keyword_3 = ['maternity']
tf_keyword_3

# Instantiate
vectorizer = TfidfVectorizer()
# Fit the data
tfidf_4 = vectorizer.fit_transform(df_4['NN_Description'])
# Create a dataframe of TFIDF
tfidf_df_4 = pd.DataFrame(tfidf_4[0].T.todense(),
                      index=vectorizer.get_feature_names_out(),
                      columns=["TF-IDF"])
# Sort
tfidf_df_4 = tfidf_df_4.sort_values('TF-IDF', ascending=False)
tfidf_df_4[:20].plot.bar(title="Top 30 TF-IDF Words")

tf_keyword_4 = ['badge']
tf_keyword_4

topics =[tf_keyword_0,tf_keyword_1,tf_keyword_2,tf_keyword_3,tf_keyword_4]

def WordnetShortestPath_labelscore(a):
  lowest_netavg=100
  lowest_label=0
  label=-1
  #print(a)
  words = nltk.word_tokenize(a)
  for z in topics:
    total=0
    counter=0
    #print(z)
    for x in z:
      count=0
      avg=0
      sum=0
      for y in words:
        if(wordnet.synsets(x) and wordnet.synsets(y)):
          syn1 = wordnet.synsets(x)[0]
          syn2 = wordnet.synsets(y)[0]
          if(syn1.pos() == 'n' and syn2.pos() == 'n'):
            #print("Shortest path between ",x," and ",y," is: ", syn1.shortest_path_distance(syn2))
            sum=sum+syn1.shortest_path_distance(syn2)
            count=count+1
      if(count==0):
        avg=0
      else:
        avg=sum/count
      total=total+avg
      counter=counter+1
      #print(sum)
      #print(count)
      #print(avg)
    if(counter==0):
      netavg=0
    else:
      netavg=total/counter
    #print(counter)
    #print("Total score for ",z," is: ",total)
    #print("Net average for ",z," is: ",netavg)
    label=label+1
    if(netavg<lowest_netavg):
      lowest_netavg=netavg
      lowest_label=label
      #print("label: ",label," has shortest path value: ",lowest_netavg)
  return lowest_label,lowest_netavg

df[ ["WSP_Label","WSP_Value"] ] = df["Description"].apply(WordnetShortestPath_labelscore).apply(pd.Series)

df[ ["NN_WSP_Label","NN_WSP_Value"] ] = df["NN_Description"].apply(WordnetShortestPath_labelscore).apply(pd.Series)

df["WSP_Label"] = df["WSP_Label"].astype(int)

df["NN_WSP_Label"] = df["NN_WSP_Label"].astype(int)

df

accuracy_score(df['Label'], df['WSP_Label'])

accuracy_score(df['Label'], df['NN_WSP_Label'])

df_0 = df.loc[(df['Label'] == 0)]
df_1 = df.loc[(df['Label'] == 1)]
df_2 = df.loc[(df['Label'] == 2)]
df_3 = df.loc[(df['Label'] == 3)]
df_4 = df.loc[(df['Label'] == 4)]

accuracy_score(df_0['Label'], df_0['WSP_Label'])

accuracy_score(df_0['Label'], df_0['NN_WSP_Label'])

accuracy_score(df_1['Label'], df_1['WSP_Label'])

accuracy_score(df_1['Label'], df_1['NN_WSP_Label'])

accuracy_score(df_2['Label'], df_2['WSP_Label'])

accuracy_score(df_2['Label'], df_2['NN_WSP_Label'])

accuracy_score(df_3['Label'], df_3['WSP_Label'])

accuracy_score(df_3['Label'], df_3['NN_WSP_Label'])

accuracy_score(df_4['Label'], df_4['WSP_Label'])

accuracy_score(df_4['Label'], df_4['NN_WSP_Label'])

def WordnetWUP_labelscore(a):
  highest_netavg=0
  highest_label=0
  label=-1
  #print(a)
  words = nltk.word_tokenize(a)
  for z in topics:
    total=0
    counter=0
    #print(z)
    for x in z:
      count=0
      avg=0
      sum=0
      for y in words:
        if(wordnet.synsets(x) and wordnet.synsets(y)):
          syn1 = wordnet.synsets(x)[0]
          syn2 = wordnet.synsets(y)[0]
          if(syn1.pos() == 'n' and syn2.pos() == 'n'):
            #print("Shortest path between ",x," and ",y," is: ", syn1.wup_similarity(syn2))
            sum=sum+syn1.wup_similarity(syn2)
            count=count+1
      if(count==0):
        avg=0
      else:
        avg=sum/count
      total=total+avg
      counter=counter+1
      #print(sum)
      #print(count)
      #print(avg)
    if(counter==0):
      netavg=0
    else:
      netavg=total/counter
    #print(counter)
    #print("Total score for ",z," is: ",total)
    #print("Net average for ",z," is: ",netavg)
    label=label+1
    if(netavg>highest_netavg):
      highest_netavg=netavg
      highest_label=label
      #print("label: ",label," has shortest path value: ",lowest_netavg)
  return highest_label,highest_netavg

df[ ["WUP_Label","WUP_Value"] ] = df["Description"].apply(WordnetWUP_labelscore).apply(pd.Series)

df[ ["NN_WUP_Label","NN_WUP_Value"] ] = df["NN_Description"].apply(WordnetWUP_labelscore).apply(pd.Series)

df["WUP_Label"] = df["WUP_Label"].astype(int)

df["NN_WUP_Label"] = df["NN_WUP_Label"].astype(int)

df

df_0 = df.loc[(df['Label'] == 0)]
df_1 = df.loc[(df['Label'] == 1)]
df_2 = df.loc[(df['Label'] == 2)]
df_3 = df.loc[(df['Label'] == 3)]
df_4 = df.loc[(df['Label'] == 4)]

accuracy_score(df['Label'], df['WUP_Label'])

accuracy_score(df['Label'], df['NN_WUP_Label'])

accuracy_score(df_0['Label'], df_0['WUP_Label'])

accuracy_score(df_0['Label'], df_0['NN_WUP_Label'])

accuracy_score(df_1['Label'], df_1['WUP_Label'])

accuracy_score(df_1['Label'], df_1['NN_WUP_Label'])

accuracy_score(df_2['Label'], df_2['WUP_Label'])

accuracy_score(df_2['Label'], df_2['NN_WUP_Label'])

accuracy_score(df_3['Label'], df_3['WUP_Label'])

accuracy_score(df_3['Label'], df_3['NN_WUP_Label'])

accuracy_score(df_4['Label'], df_4['WUP_Label'])

accuracy_score(df_4['Label'], df_4['NN_WUP_Label'])