{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7-knfoj9cef",
        "outputId": "303d6cef-71fc-402a-e413-e89a08d4b558"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yake in /usr/local/lib/python3.10/dist-packages (0.4.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from yake) (0.9.0)\n",
            "Requirement already satisfied: click>=6.0 in /usr/local/lib/python3.10/dist-packages (from yake) (8.1.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from yake) (1.25.2)\n",
            "Requirement already satisfied: segtok in /usr/local/lib/python3.10/dist-packages (from yake) (1.5.11)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from yake) (3.2.1)\n",
            "Requirement already satisfied: jellyfish in /usr/local/lib/python3.10/dist-packages (from yake) (1.0.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segtok->yake) (2023.12.25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfDodKJL1NH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e9e2f8f-c2bc-43fe-cf57-cb34ebed6614"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet_ic')\n",
        "from nltk.corpus import wordnet_ic\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import wordnet\n",
        "from itertools import chain\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import yake"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('latest_ticket_data.csv')"
      ],
      "metadata": {
        "id": "zkr0Kc7R2pGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_only_chars(line):\n",
        "\n",
        "    clean_line = \"\"\n",
        "\n",
        "    line = line.replace(\"â€™\", \"\")\n",
        "    line = line.replace(\"'\", \"\")\n",
        "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
        "    line = line.replace(\"\\t\", \" \")\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    line = line.lower()\n",
        "\n",
        "    for char in line:\n",
        "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
        "            clean_line += char\n",
        "        else:\n",
        "            clean_line += ' '\n",
        "\n",
        "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
        "    if clean_line[0] == ' ':\n",
        "        clean_line = clean_line[1:]\n",
        "    return clean_line"
      ],
      "metadata": {
        "id": "wKt5AnR-TZav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Description'] = df['Description'].apply(lambda x: get_only_chars(x))"
      ],
      "metadata": {
        "id": "OuDZi8vXTkKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tokenized']=[nltk.word_tokenize(i) for i in df['Description']]"
      ],
      "metadata": {
        "id": "iB0EJWU63XKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['pos_tagged'] = [nltk.pos_tag(i) for i in df['Tokenized']]"
      ],
      "metadata": {
        "id": "tcXSr0ye3b5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['NN_tagged'] = df['pos_tagged'].apply(lambda item:[w for w,t in item if t=='NN'])"
      ],
      "metadata": {
        "id": "k8OsXCby3iwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['NN_Description'] = df.NN_tagged.map(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "insbKP9b3nCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "custom_stop_words = ['hi', 'since', 'please', 'best', 'regards', 'thank', 'thanks', 'hello', 'sent', 'great', 'dear', 'help', 'kind']\n",
        "time_words = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december',\n",
        "              'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'today' , 'yesterday', 'tomorrow',\n",
        "              'hour', 'hours', 'time', 'times', 'timelines', 'date', 'day', 'days', 'am', 'pm', 'morning', 'noon', 'afternoon', 'evening',\n",
        "              'night', 'winter', 'summer', 'rain', 'cold']\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "    return text\n",
        "\n",
        "def remove_custom_words(text):\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(custom_stop_words) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "    return text\n",
        "\n",
        "def remove_time_words(text):\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(time_words) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "    return text\n",
        "\n",
        "df['NN_Description'] = df[\"NN_Description\"].map(lambda x: remove_stop_words(x))\n",
        "df['NN_Description'] = df[\"NN_Description\"].map(lambda x: remove_custom_words(x))\n",
        "df['NN_Description'] = df[\"NN_Description\"].map(lambda x: remove_time_words(x))"
      ],
      "metadata": {
        "id": "ZX8Uiht6ULEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LE = LabelEncoder()\n",
        "df['label'] = LE.fit_transform(df['Category'])"
      ],
      "metadata": {
        "id": "8asUEwKY2sK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_0 = df.loc[(df['label'] == 0)]\n",
        "df_1 = df.loc[(df['label'] == 1)]\n",
        "df_2 = df.loc[(df['label'] == 2)]\n",
        "df_3 = df.loc[(df['label'] == 3)]\n",
        "df_4 = df.loc[(df['label'] == 4)]"
      ],
      "metadata": {
        "id": "Vo6nNDag9pqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kw_extractor = yake.KeywordExtractor(n=1)"
      ],
      "metadata": {
        "id": "f3OjEfl2CgpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = df_0['NN_Description'].apply(kw_extractor.extract_keywords)\n",
        "# Extract scores from the YAKE output\n",
        "text = []\n",
        "for list in keywords:\n",
        "    for component in list:\n",
        "        text.append(str(component[0]))\n",
        "\n",
        "# Bar plot - Create a dataframe of the most common 20 words\n",
        "common_words = pd.DataFrame(Counter(text).most_common(20))\n",
        "common_words.columns = ('Word', 'Count')\n",
        "keyword_0 = [common_words['Word'][0],common_words['Word'][1], common_words['Word'][2],common_words['Word'][3],common_words['Word'][4]]"
      ],
      "metadata": {
        "id": "L_Gc9svZ9vKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = df_1['NN_Description'].apply(kw_extractor.extract_keywords)\n",
        "# Extract scores from the YAKE output\n",
        "text = []\n",
        "for list in keywords:\n",
        "    for component in list:\n",
        "        text.append(str(component[0]))\n",
        "\n",
        "# Bar plot - Create a dataframe of the most common 20 words\n",
        "common_words = pd.DataFrame(Counter(text).most_common(20))\n",
        "common_words.columns = ('Word', 'Count')\n",
        "keyword_1 = [common_words['Word'][0],common_words['Word'][1], common_words['Word'][2],common_words['Word'][3],common_words['Word'][4]]"
      ],
      "metadata": {
        "id": "gi_PQhpPEAgS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = df_2['NN_Description'].apply(kw_extractor.extract_keywords)\n",
        "# Extract scores from the YAKE output\n",
        "text = []\n",
        "for list in keywords:\n",
        "    for component in list:\n",
        "        text.append(str(component[0]))\n",
        "\n",
        "# Bar plot - Create a dataframe of the most common 20 words\n",
        "common_words = pd.DataFrame(Counter(text).most_common(20))\n",
        "common_words.columns = ('Word', 'Count')\n",
        "keyword_2 = [common_words['Word'][0],common_words['Word'][1], common_words['Word'][2],common_words['Word'][3],common_words['Word'][4]]"
      ],
      "metadata": {
        "id": "PNbuO97ZEDUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = df_3['NN_Description'].apply(kw_extractor.extract_keywords)\n",
        "# Extract scores from the YAKE output\n",
        "text = []\n",
        "for list in keywords:\n",
        "    for component in list:\n",
        "        text.append(str(component[0]))\n",
        "\n",
        "# Bar plot - Create a dataframe of the most common 20 words\n",
        "common_words = pd.DataFrame(Counter(text).most_common(20))\n",
        "common_words.columns = ('Word', 'Count')\n",
        "keyword_3 = [common_words['Word'][0],common_words['Word'][1], common_words['Word'][2],common_words['Word'][3],common_words['Word'][4]]"
      ],
      "metadata": {
        "id": "AByDsvz0EGPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = df_4['NN_Description'].apply(kw_extractor.extract_keywords)\n",
        "# Extract scores from the YAKE output\n",
        "text = []\n",
        "for list in keywords:\n",
        "    for component in list:\n",
        "        text.append(str(component[0]))\n",
        "\n",
        "# Bar plot - Create a dataframe of the most common 20 words\n",
        "common_words = pd.DataFrame(Counter(text).most_common(20))\n",
        "common_words.columns = ('Word', 'Count')\n",
        "keyword_4 = [common_words['Word'][0],common_words['Word'][1], common_words['Word'][2],common_words['Word'][3],common_words['Word'][4]]"
      ],
      "metadata": {
        "id": "4Ka5us1gEJCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = [keyword_0,keyword_1,keyword_2,keyword_3,keyword_4]"
      ],
      "metadata": {
        "id": "XU3LBPCcETEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelList = df['Category'].unique().tolist()"
      ],
      "metadata": {
        "id": "VShSKA_3qgea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate samples that contains K samples of each class\n",
        "def gen_sample(df, sample_size, num_classes):\n",
        "\n",
        "    df_1 = df[(df[\"label\"]<num_classes + 1)].reset_index().drop([\"index\"], axis=1).reset_index().drop([\"index\"], axis=1)\n",
        "    train = df_1[df_1[\"label\"] == np.unique(df_1['label'])[0]].sample(sample_size)\n",
        "\n",
        "    train_index = train.index.tolist()\n",
        "\n",
        "    for i in range(1,num_classes):\n",
        "        train_2 = df_1[df_1[\"label\"] == np.unique(df_1['label'])[i]].sample(sample_size)\n",
        "        train = pd.concat([train, train_2], axis=0)\n",
        "        train_index.extend(train_2.index.tolist())\n",
        "\n",
        "    test = df_1[~df_1.index.isin(train_index)]\n",
        "\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "FoBJD13Rku67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetShortestPath_labelscore(a,topics):\n",
        "  lowest_netavg=100\n",
        "  lowest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.shortest_path_distance(syn2))\n",
        "            sum=sum+syn1.shortest_path_distance(syn2)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg<lowest_netavg):\n",
        "      lowest_netavg=netavg\n",
        "      lowest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return lowest_label,lowest_netavg"
      ],
      "metadata": {
        "id": "saMjRxel4BqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetLeacock_labelscore(a,topics):\n",
        "  lowest_netavg=100\n",
        "  lowest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.shortest_path_distance(syn2))\n",
        "            sum=sum+syn1.lch_similarity(syn2)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg<lowest_netavg):\n",
        "      lowest_netavg=netavg\n",
        "      lowest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return lowest_label,lowest_netavg"
      ],
      "metadata": {
        "id": "v5u9bQVhqVQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetRES_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            sum=sum+syn1.res_similarity(syn2, brown_ic)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ],
      "metadata": {
        "id": "KXfGRH2LnkxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetLIN_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.wup_similarity(syn2))\n",
        "            sum=sum+syn1.lin_similarity(syn2, brown_ic)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ],
      "metadata": {
        "id": "5V_Qpb68thZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetJCN_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.wup_similarity(syn2))\n",
        "            sum=sum+syn1.jcn_similarity(syn2, brown_ic)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ],
      "metadata": {
        "id": "x7EpXseYtMCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetWUP_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.wup_similarity(syn2))\n",
        "            sum=sum+syn1.wup_similarity(syn2)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ],
      "metadata": {
        "id": "LihS7434Nqgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ZeroShotWordnetModel(text, labelList, keywordstobeGenerated='Yes', keywordsList=[], keywordsType='synonym', posfilterType='NN', measureType='ShortestPath'):\n",
        "  if not text and not labelList:\n",
        "    print(\"Input Text and List of Label/Category names required\")\n",
        "    return()\n",
        "  else:\n",
        "    if(keywordstobeGenerated=='Yes'):\n",
        "      topics = getKeywords(labelList,keywordsType)\n",
        "    else:\n",
        "      topics = keywordsList\n",
        "    if(posfilterType=='NN'):\n",
        "      if(measureType=='WuPalmer'):\n",
        "        return(WordnetWUP_labelscore(text,topics))\n",
        "      if(measureType=='Resnik'):\n",
        "        return(WordnetRES_labelscore(text,topics))\n",
        "      if(measureType=='JCN'):\n",
        "        return(WordnetJCN_labelscore(text,topics))\n",
        "      if(measureType=='Lin'):\n",
        "        return(WordnetLIN_labelscore(text,topics))\n",
        "      if(measureType=='Leacock'):\n",
        "        return(WordnetLeacock_labelscore(text,topics))\n",
        "      if(measureType=='ShortestPath'):\n",
        "        return(WordnetShortestPath_labelscore(text,topics))\n"
      ],
      "metadata": {
        "id": "LQqGTWxPspyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, rest = gen_sample(df, 100, 5)"
      ],
      "metadata": {
        "id": "tmmAqSOak0vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_Leacock_Label', 'NN_Leacock_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,keywordstobeGenerated='No',keywordsList=topics,measureType='Leacock')))"
      ],
      "metadata": {
        "id": "qHPJNWfW3QmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_Leacock_Label\"] = df[\"NN_Leacock_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "v2Jn-Oew-PkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_Leacock_Label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osbu7UnG-VGc",
        "outputId": "c1ba534c-12b9-4602-dd4f-34a20be6ade6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12433333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_Lin_Label', 'NN_Lin_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,keywordstobeGenerated='No',keywordsList=topics,measureType='Lin')))"
      ],
      "metadata": {
        "id": "Ik4m-QJY7NRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_Lin_Label\"] = df[\"NN_Lin_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "ikVFO_l0_RtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_Lin_Label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwv0Zb-1_Uzp",
        "outputId": "dfbd52b4-db1e-40ae-c336-a60c60e2ac63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3883333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_JCN_Label', 'NN_JCN_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,keywordstobeGenerated='No',keywordsList=topics,measureType='JCN')))"
      ],
      "metadata": {
        "id": "1WGPi0GI7ntD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_JCN_Label\"] = df[\"NN_JCN_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "oJyn65aj_aeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_JCN_Label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCSMW2fk_bMv",
        "outputId": "e154132a-469f-4780-bc24-30296305abea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.423"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_Resnik_Label', 'NN_Resnik_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,keywordstobeGenerated='No',keywordsList=topics,measureType='Resnik')))"
      ],
      "metadata": {
        "id": "5ctvZe8i7xKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_Resnik_Label\"] = df[\"NN_Resnik_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "5sa3gcOy_iB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_Resnik_Label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL_ayQq8ACa3",
        "outputId": "23f023e2-608d-49a5-d66e-aa14646d9f4f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.36666666666666664"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_WUP_Label', 'NN_WUP_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,keywordstobeGenerated='No',keywordsList=topics,measureType='WuPalmer')))"
      ],
      "metadata": {
        "id": "iZZpwWOA8TUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_WUP_Label\"] = df[\"NN_WUP_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "gQ3lWNGZAA_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_WUP_Label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcWFehVpAFSq",
        "outputId": "fdf6c623-cdcd-4e00-b90d-70dd0a075c48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.35833333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_WSP_Label', 'NN_WSP_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,keywordstobeGenerated='No',keywordsList=topics)))"
      ],
      "metadata": {
        "id": "RbC17Tx3HKpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_WSP_Label\"] = df[\"NN_WSP_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "2J8jXVYc_sZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_WSP_Label'])"
      ],
      "metadata": {
        "id": "9V_5rDwzNbaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83406a4c-3e82-460c-b5b6-a4e2a41bc4a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.29333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    }
  ]
}