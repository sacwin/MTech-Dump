# -*- coding: utf-8 -*-
"""WSTM_WordCount.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DC9atm9mo8wlkSeURzYlVoDWDAhnXsRj
"""

# Import Libraries
import pandas as pd
import io
import numpy as np
import re

from collections import Counter
import seaborn as sns
import matplotlib.pyplot as plt

import nltk
nltk.download('wordnet')
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('stopwords')
from nltk.corpus import wordnet
from nltk.corpus import stopwords

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

df = pd.read_csv('latest_ticket_data.csv')

def get_only_chars(line):

    clean_line = ""

    line = line.replace("â€™", "")
    line = line.replace("'", "")
    line = line.replace("-", " ") #replace hyphens with spaces
    line = line.replace("\t", " ")
    line = line.replace("\n", " ")
    line = line.lower()

    for char in line:
        if char in 'qwertyuiopasdfghjklzxcvbnm ':
            clean_line += char
        else:
            clean_line += ' '

    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces
    if clean_line[0] == ' ':
        clean_line = clean_line[1:]
    return clean_line

df['Description'] = df['Description'].apply(lambda x: get_only_chars(x))

df['Tokenized']=[nltk.word_tokenize(i) for i in df['Description']]

df['pos_tagged'] = [nltk.pos_tag(i) for i in df['Tokenized']]

df['NN_tagged'] = df['pos_tagged'].apply(lambda item:[w for w,t in item if t=='NN'])

df['NN_Description'] = df.NN_tagged.map(lambda x: ' '.join(x))

stop_words = stopwords.words('english')
custom_stop_words = ['hi', 'since', 'please', 'best', 'regards', 'thank', 'thanks', 'hello', 'sent', 'great', 'dear', 'help', 'kind']
time_words = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december',
              'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'today' , 'yesterday', 'tomorrow',
              'hour', 'hours', 'time', 'times', 'timelines', 'date', 'day', 'days', 'am', 'pm', 'morning', 'noon', 'afternoon', 'evening',
              'night', 'winter', 'summer', 'rain', 'cold']

def remove_stop_words(text):
    pattern = re.compile(r'\b(' + r'|'.join(stop_words) + r')\b\s*')
    text = pattern.sub('', text)
    return text

def remove_custom_words(text):
    pattern = re.compile(r'\b(' + r'|'.join(custom_stop_words) + r')\b\s*')
    text = pattern.sub('', text)
    return text

def remove_time_words(text):
    pattern = re.compile(r'\b(' + r'|'.join(time_words) + r')\b\s*')
    text = pattern.sub('', text)
    return text

df['NN_Description'] = df["NN_Description"].map(lambda x: remove_stop_words(x))
df['NN_Description'] = df["NN_Description"].map(lambda x: remove_custom_words(x))
df['NN_Description'] = df["NN_Description"].map(lambda x: remove_time_words(x))

LE = LabelEncoder()
df['Label'] = LE.fit_transform(df['Category'])

print(df)

df_0 = df.loc[(df['Label'] == 0)]
df_1 = df.loc[(df['Label'] == 1)]
df_2 = df.loc[(df['Label'] == 2)]
df_3 = df.loc[(df['Label'] == 3)]
df_4 = df.loc[(df['Label'] == 4)]

corpus = []
for disclosures in df_0['NN_Description'].tolist():
    for word in disclosures.split():
        corpus.append(word)
# Bar plot - Create a dataframe of the most common 30 words
common_words = pd.DataFrame(Counter(corpus).most_common(20))
common_words.columns = ('Word', 'Count')
keyword_0 = [common_words['Word'][0],common_words['Word'][1], common_words['Word'][2],common_words['Word'][3],common_words['Word'][4]]
keyword_0
# Plot a bar chart of the most common 20 words
#sns.set(font_scale = 1.5)
#sns.barplot(x= common_words['Word'], y=common_words['Count'])
#plt.xticks(rotation='vertical')
#plt.title("Key Word Count", fontsize = 20)
#plt.show()

corpus = []
for disclosures in df_1['NN_Description'].tolist():
    for word in disclosures.split():
        corpus.append(word)
# Bar plot - Create a dataframe of the most common 30 words
common_words = pd.DataFrame(Counter(corpus).most_common(20))
common_words.columns = ('Word', 'Count')
keyword_1 = [common_words['Word'][0],common_words['Word'][1], common_words['Word'][2],common_words['Word'][3],common_words['Word'][4]]
keyword_1
# Plot a bar chart of the most common 20 words
#sns.set(font_scale = 1.5)
#sns.barplot(x= common_words['Word'], y=common_words['Count'])
#plt.xticks(rotation='vertical')
#plt.title("Key Word Count", fontsize = 20)
#plt.show()

corpus = []
for disclosures in df_2['NN_Description'].tolist():
    for word in disclosures.split():
        corpus.append(word)
# Bar plot - Create a dataframe of the most common 30 words
common_words = pd.DataFrame(Counter(corpus).most_common(20))
common_words.columns = ('Word', 'Count')
keyword_2 = [common_words['Word'][0],common_words['Word'][1], common_words['Word'][2],common_words['Word'][3],common_words['Word'][4]]
keyword_2
# Plot a bar chart of the most common 20 words
#sns.set(font_scale = 1.5)
#sns.barplot(x= common_words['Word'], y=common_words['Count'])
#plt.xticks(rotation='vertical')
#plt.title("Key Word Count", fontsize = 20)
#plt.show()

corpus = []
for disclosures in df_3['NN_Description'].tolist():
    for word in disclosures.split():
        corpus.append(word)
# Bar plot - Create a dataframe of the most common 30 words
common_words = pd.DataFrame(Counter(corpus).most_common(20))
common_words.columns = ('Word', 'Count')
keyword_3 = [common_words['Word'][0],common_words['Word'][1], common_words['Word'][2],common_words['Word'][3],common_words['Word'][4]]
keyword_3
# Plot a bar chart of the most common 20 words
#sns.set(font_scale = 1.5)
#sns.barplot(x= common_words['Word'], y=common_words['Count'])
#plt.xticks(rotation='vertical')
#plt.title("Key Word Count", fontsize = 20)
#plt.show()

corpus = []
for disclosures in df_4['NN_Description'].tolist():
    for word in disclosures.split():
        corpus.append(word)
# Bar plot - Create a dataframe of the most common 30 words
common_words = pd.DataFrame(Counter(corpus).most_common(20))
common_words.columns = ('Word', 'Count')
keyword_4 = [common_words['Word'][0],common_words['Word'][1], common_words['Word'][2],common_words['Word'][3],common_words['Word'][4]]
keyword_4
# Plot a bar chart of the most common 20 words
#sns.set(font_scale = 1.5)
#sns.barplot(x= common_words['Word'], y=common_words['Count'])
#plt.xticks(rotation='vertical')
#plt.title("Key Word Count", fontsize = 20)
#plt.show()

topics = [keyword_0,keyword_1,keyword_2,keyword_3,keyword_4]

def WordnetShortestPath_labelscore(a):
  lowest_netavg=100
  lowest_label=0
  label=-1
  #print(a)
  words = nltk.word_tokenize(a)
  for z in topics:
    total=0
    counter=0
    #print(z)
    for x in z:
      count=0
      avg=0
      sum=0
      for y in words:
        if(wordnet.synsets(x) and wordnet.synsets(y)):
          syn1 = wordnet.synsets(x)[0]
          syn2 = wordnet.synsets(y)[0]
          if(syn1.pos() == 'n' and syn2.pos() == 'n'):
            #print("Shortest path between ",x," and ",y," is: ", syn1.shortest_path_distance(syn2))
            sum=sum+syn1.shortest_path_distance(syn2)
            count=count+1
      if(count==0):
        avg=0
      else:
        avg=sum/count
      total=total+avg
      counter=counter+1
      #print(sum)
      #print(count)
      #print(avg)
    if(counter==0):
      netavg=0
    else:
      netavg=total/counter
    #print(counter)
    #print("Total score for ",z," is: ",total)
    #print("Net average for ",z," is: ",netavg)
    label=label+1
    if(netavg<lowest_netavg):
      lowest_netavg=netavg
      lowest_label=label
      #print("label: ",label," has shortest path value: ",lowest_netavg)
  return lowest_label,lowest_netavg

df[ ["WSP_Label","WSP_Value"] ] = df["Description"].apply(WordnetShortestPath_labelscore).apply(pd.Series)

df[ ["NN_WSP_Label","NN_WSP_Value"] ] = df["NN_Description"].apply(WordnetShortestPath_labelscore).apply(pd.Series)

df["WSP_Label"] = df["WSP_Label"].astype(int)

df["NN_WSP_Label"] = df["NN_WSP_Label"].astype(int)

df

df_0 = df.loc[(df['Label'] == 0)]
df_1 = df.loc[(df['Label'] == 1)]
df_2 = df.loc[(df['Label'] == 2)]
df_3 = df.loc[(df['Label'] == 3)]
df_4 = df.loc[(df['Label'] == 4)]

accuracy_score(df['Label'], df['WSP_Label'])

accuracy_score(df['Label'], df['NN_WSP_Label'])

accuracy_score(df_0['Label'], df_0['WSP_Label'])

accuracy_score(df_0['Label'], df_0['NN_WSP_Label'])

accuracy_score(df_1['Label'], df_1['WSP_Label'])

accuracy_score(df_1['Label'], df_1['NN_WSP_Label'])

accuracy_score(df_2['Label'], df_2['WSP_Label'])

accuracy_score(df_2['Label'], df_2['NN_WSP_Label'])

accuracy_score(df_2['Label'], df_2['NN_WSP_Label'])

accuracy_score(df_3['Label'], df_3['WSP_Label'])

accuracy_score(df_3['Label'], df_3['NN_WSP_Label'])

accuracy_score(df_4['Label'], df_4['WSP_Label'])

accuracy_score(df_4['Label'], df_4['NN_WSP_Label'])

def WordnetWUP_labelscore(a):
  highest_netavg=0
  highest_label=0
  label=-1
  #print(a)
  words = nltk.word_tokenize(a)
  for z in topics:
    total=0
    counter=0
    #print(z)
    for x in z:
      count=0
      avg=0
      sum=0
      for y in words:
        if(wordnet.synsets(x) and wordnet.synsets(y)):
          syn1 = wordnet.synsets(x)[0]
          syn2 = wordnet.synsets(y)[0]
          if(syn1.pos() == 'n' and syn2.pos() == 'n'):
            #print("Shortest path between ",x," and ",y," is: ", syn1.wup_similarity(syn2))
            sum=sum+syn1.wup_similarity(syn2)
            count=count+1
      if(count==0):
        avg=0
      else:
        avg=sum/count
      total=total+avg
      counter=counter+1
      #print(sum)
      #print(count)
      #print(avg)
    if(counter==0):
      netavg=0
    else:
      netavg=total/counter
    #print(counter)
    #print("Total score for ",z," is: ",total)
    #print("Net average for ",z," is: ",netavg)
    label=label+1
    if(netavg>highest_netavg):
      highest_netavg=netavg
      highest_label=label
      #print("label: ",label," has shortest path value: ",lowest_netavg)
  return highest_label,highest_netavg

df[ ["WUP_Label","WUP_Value"] ] = df["Description"].apply(WordnetWUP_labelscore).apply(pd.Series)

df[ ["NN_WUP_Label","NN_WUP_Value"] ] = df["NN_Description"].apply(WordnetWUP_labelscore).apply(pd.Series)

df["WUP_Label"] = df["WUP_Label"].astype(int)

df["NN_WUP_Label"] = df["NN_WUP_Label"].astype(int)

df

df_0 = df.loc[(df['Label'] == 0)]
df_1 = df.loc[(df['Label'] == 1)]
df_2 = df.loc[(df['Label'] == 2)]
df_3 = df.loc[(df['Label'] == 3)]
df_4 = df.loc[(df['Label'] == 4)]

accuracy_score(df['Label'], df['WUP_Label'])

accuracy_score(df['Label'], df['NN_WUP_Label'])

accuracy_score(df_0['Label'], df_0['WUP_Label'])

accuracy_score(df_0['Label'], df_0['NN_WUP_Label'])

accuracy_score(df_1['Label'], df_1['WUP_Label'])

accuracy_score(df_1['Label'], df_1['NN_WUP_Label'])

accuracy_score(df_2['Label'], df_2['WUP_Label'])

accuracy_score(df_2['Label'], df_2['NN_WUP_Label'])

accuracy_score(df_3['Label'], df_3['WUP_Label'])

accuracy_score(df_3['Label'], df_3['NN_WUP_Label'])

accuracy_score(df_4['Label'], df_4['WUP_Label'])

accuracy_score(df_4['Label'], df_4['NN_WUP_Label'])