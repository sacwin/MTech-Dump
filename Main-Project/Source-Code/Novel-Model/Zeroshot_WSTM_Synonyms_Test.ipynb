{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfDodKJL1NH2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "305cb138-762f-42e8-e4e3-313951107d18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet_ic')\n",
        "from nltk.corpus import wordnet_ic\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import wordnet\n",
        "from itertools import chain\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('latest_ticket_data.csv')"
      ],
      "metadata": {
        "id": "zkr0Kc7R2pGa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_only_chars(line):\n",
        "\n",
        "    clean_line = \"\"\n",
        "\n",
        "    line = line.replace(\"â€™\", \"\")\n",
        "    line = line.replace(\"'\", \"\")\n",
        "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
        "    line = line.replace(\"\\t\", \" \")\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    line = line.lower()\n",
        "\n",
        "    for char in line:\n",
        "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
        "            clean_line += char\n",
        "        else:\n",
        "            clean_line += ' '\n",
        "\n",
        "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
        "    if clean_line[0] == ' ':\n",
        "        clean_line = clean_line[1:]\n",
        "    return clean_line"
      ],
      "metadata": {
        "id": "wKt5AnR-TZav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Description'] = df['Description'].apply(lambda x: get_only_chars(x))"
      ],
      "metadata": {
        "id": "OuDZi8vXTkKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Tokenized']=[nltk.word_tokenize(i) for i in df['Description']]"
      ],
      "metadata": {
        "id": "iB0EJWU63XKZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['pos_tagged'] = [nltk.pos_tag(i) for i in df['Tokenized']]"
      ],
      "metadata": {
        "id": "tcXSr0ye3b5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['NN_tagged'] = df['pos_tagged'].apply(lambda item:[w for w,t in item if t=='NN'])"
      ],
      "metadata": {
        "id": "k8OsXCby3iwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['NN_Description'] = df.NN_tagged.map(lambda x: ' '.join(x))"
      ],
      "metadata": {
        "id": "insbKP9b3nCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "custom_stop_words = ['hi', 'since', 'please', 'best', 'regards', 'thank', 'thanks', 'hello', 'sent', 'great', 'dear', 'help', 'kind']\n",
        "time_words = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december',\n",
        "              'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'today' , 'yesterday', 'tomorrow',\n",
        "              'hour', 'hours', 'time', 'times', 'timelines', 'date', 'day', 'days', 'am', 'pm', 'morning', 'noon', 'afternoon', 'evening',\n",
        "              'night', 'winter', 'summer', 'rain', 'cold']\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "    return text\n",
        "\n",
        "def remove_custom_words(text):\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(custom_stop_words) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "    return text\n",
        "\n",
        "def remove_time_words(text):\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(time_words) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "    return text\n",
        "\n",
        "df['NN_Description'] = df[\"NN_Description\"].map(lambda x: remove_stop_words(x))\n",
        "df['NN_Description'] = df[\"NN_Description\"].map(lambda x: remove_custom_words(x))\n",
        "df['NN_Description'] = df[\"NN_Description\"].map(lambda x: remove_time_words(x))"
      ],
      "metadata": {
        "id": "ZX8Uiht6ULEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LE = LabelEncoder()\n",
        "df['label'] = LE.fit_transform(df['Category'])"
      ],
      "metadata": {
        "id": "8asUEwKY2sK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Get_Label_Synonyms(label):\n",
        "  result=label.split(\" \")\n",
        "  wordcount=len(result)\n",
        "  synlist = []\n",
        "  if(wordcount == 1):\n",
        "    if(wordnet.synsets(label)):\n",
        "      synonyms = wordnet.synsets(label)\n",
        "      synlist = list(set(chain.from_iterable([word.lemma_names() for word in synonyms])))\n",
        "  else:\n",
        "    for j in range(0,wordcount):\n",
        "      label_j = []\n",
        "      if(wordnet.synsets(result[j])):\n",
        "        synonyms = wordnet.synsets(result[j])\n",
        "        label_j = list(set(chain.from_iterable([word.lemma_names() for word in synonyms])))\n",
        "        synlist = synlist + label_j\n",
        "  synlist = [item.replace(\"_\",\" \") for item in synlist]\n",
        "  return(synlist)"
      ],
      "metadata": {
        "id": "-KKh_mRJsBy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labelList = df['Category'].unique().tolist()"
      ],
      "metadata": {
        "id": "VShSKA_3qgea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getSynonymKeywords(labelList):\n",
        "  length=len(labelList)\n",
        "  topics=[]\n",
        "  for i in range(0,length):\n",
        "    label_i = Get_Label_Synonyms(labelList[i])\n",
        "    t = ' '.join(str(x) for x in label_i)\n",
        "    u = ' '.join(set(t.split()))\n",
        "    topics.append(u)\n",
        "  return(topics)"
      ],
      "metadata": {
        "id": "cJxJxwC0ymvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getKeywords(labelList,keywordsType):\n",
        "  topics=[]\n",
        "  if(keywordsType=='synonym'):\n",
        "    topics=getSynonymKeywords(labelList)\n",
        "    return(topics)"
      ],
      "metadata": {
        "id": "ZUIYYPuoNMDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topics = getKeywords(labelList,'synonym')\n",
        "print(topics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bUg-C8q69A",
        "outputId": "74b0f337-8fa7-4d35-c6a2-d66eba8aedde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lotion covering diligence program practical applications coating application programme', 'database', 'web net mesh electronic meshwork network meshing', 'sustenance sustainment user drug criminal care maintenance upkeep substance exploiter sustentation alimony abuser', 'protection system measures measure department certificate surety security']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate samples that contains K samples of each class\n",
        "def gen_sample(df, sample_size, num_classes):\n",
        "\n",
        "    df_1 = df[(df[\"label\"]<num_classes + 1)].reset_index().drop([\"index\"], axis=1).reset_index().drop([\"index\"], axis=1)\n",
        "    train = df_1[df_1[\"label\"] == np.unique(df_1['label'])[0]].sample(sample_size)\n",
        "\n",
        "    train_index = train.index.tolist()\n",
        "\n",
        "    for i in range(1,num_classes):\n",
        "        train_2 = df_1[df_1[\"label\"] == np.unique(df_1['label'])[i]].sample(sample_size)\n",
        "        train = pd.concat([train, train_2], axis=0)\n",
        "        train_index.extend(train_2.index.tolist())\n",
        "\n",
        "    test = df_1[~df_1.index.isin(train_index)]\n",
        "\n",
        "    return train, test"
      ],
      "metadata": {
        "id": "FoBJD13Rku67"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetShortestPath_labelscore(a,topics):\n",
        "  lowest_netavg=100\n",
        "  lowest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.shortest_path_distance(syn2))\n",
        "            sum=sum+syn1.shortest_path_distance(syn2)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg<lowest_netavg):\n",
        "      lowest_netavg=netavg\n",
        "      lowest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return lowest_label,lowest_netavg"
      ],
      "metadata": {
        "id": "saMjRxel4BqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetLeacock_labelscore(a,topics):\n",
        "  lowest_netavg=100\n",
        "  lowest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.shortest_path_distance(syn2))\n",
        "            sum=sum+syn1.lch_similarity(syn2)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg<lowest_netavg):\n",
        "      lowest_netavg=netavg\n",
        "      lowest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return lowest_label,lowest_netavg"
      ],
      "metadata": {
        "id": "v5u9bQVhqVQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetRES_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            sum=sum+syn1.res_similarity(syn2, brown_ic)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ],
      "metadata": {
        "id": "KXfGRH2LnkxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetLIN_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.wup_similarity(syn2))\n",
        "            sum=sum+syn1.lin_similarity(syn2, brown_ic)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ],
      "metadata": {
        "id": "5V_Qpb68thZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetJCN_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.wup_similarity(syn2))\n",
        "            sum=sum+syn1.jcn_similarity(syn2, brown_ic)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ],
      "metadata": {
        "id": "x7EpXseYtMCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def WordnetWUP_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.wup_similarity(syn2))\n",
        "            sum=sum+syn1.wup_similarity(syn2)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ],
      "metadata": {
        "id": "LihS7434Nqgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ZeroShotWordnetModel(text, labelList, keywordstobeGenerated='Yes', keywordsList=[], keywordsType='synonym', posfilterType='NN', measureType='ShortestPath'):\n",
        "  if not text and not labelList:\n",
        "    print(\"Input Text and List of Label/Category names required\")\n",
        "    return()\n",
        "  else:\n",
        "    if(keywordstobeGenerated=='Yes'):\n",
        "      topics = getKeywords(labelList,keywordsType)\n",
        "    else:\n",
        "      topics = labelList\n",
        "    if(posfilterType=='NN'):\n",
        "      if(measureType=='WuPalmer'):\n",
        "        return(WordnetWUP_labelscore(text,topics))\n",
        "      if(measureType=='Resnik'):\n",
        "        return(WordnetRES_labelscore(text,topics))\n",
        "      if(measureType=='JCN'):\n",
        "        return(WordnetJCN_labelscore(text,topics))\n",
        "      if(measureType=='Lin'):\n",
        "        return(WordnetLIN_labelscore(text,topics))\n",
        "      if(measureType=='Leacock'):\n",
        "        return(WordnetLeacock_labelscore(text,topics))\n",
        "      if(measureType=='ShortestPath'):\n",
        "        return(WordnetShortestPath_labelscore(text,topics))\n"
      ],
      "metadata": {
        "id": "LQqGTWxPspyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data, rest = gen_sample(df, 100, 5)"
      ],
      "metadata": {
        "id": "tmmAqSOak0vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_Leacock_Label', 'NN_Leacock_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,measureType='Leacock')))"
      ],
      "metadata": {
        "id": "qHPJNWfW3QmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_Leacock_Label\"] = df[\"NN_Leacock_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "v2Jn-Oew-PkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_Leacock_Label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osbu7UnG-VGc",
        "outputId": "78b4c613-adb3-4e6a-defd-ff41c4bfdc57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20066666666666666"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_Lin_Label', 'NN_Lin_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,measureType='Lin')))"
      ],
      "metadata": {
        "id": "Ik4m-QJY7NRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_Lin_Label\"] = df[\"NN_Lin_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "ikVFO_l0_RtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_Lin_Label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwv0Zb-1_Uzp",
        "outputId": "a8357578-8f4f-482c-c9b5-fdb77ab3dd59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22066666666666668"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_JCN_Label', 'NN_JCN_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,measureType='JCN')))"
      ],
      "metadata": {
        "id": "1WGPi0GI7ntD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_JCN_Label\"] = df[\"NN_JCN_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "oJyn65aj_aeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_JCN_Label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCSMW2fk_bMv",
        "outputId": "b0838e5d-d04f-469e-db68-10f507de3a2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.21133333333333335"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_Resnik_Label', 'NN_Resnik_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,measureType='Resnik')))"
      ],
      "metadata": {
        "id": "5ctvZe8i7xKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_Resnik_Label\"] = df[\"NN_Resnik_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "5sa3gcOy_iB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_Resnik_Label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL_ayQq8ACa3",
        "outputId": "cc781c5e-6638-4dad-a547-2c5a0148b4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.248"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_WUP_Label', 'NN_WUP_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,measureType='WuPalmer')))"
      ],
      "metadata": {
        "id": "iZZpwWOA8TUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_WUP_Label\"] = df[\"NN_WUP_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "gQ3lWNGZAA_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_WUP_Label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcWFehVpAFSq",
        "outputId": "ba40ce3d-96d3-49c5-92fb-47d8c675c7df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.228"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[['NN_WSP_Label', 'NN_WSP_Value']] = df['NN_Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList)))"
      ],
      "metadata": {
        "id": "RbC17Tx3HKpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"NN_WSP_Label\"] = df[\"NN_WSP_Label\"].astype(int)"
      ],
      "metadata": {
        "id": "2J8jXVYc_sZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_score(df['label'], df['NN_WSP_Label'])"
      ],
      "metadata": {
        "id": "9V_5rDwzNbaQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf4f6bc3-94f7-4ec4-a483-463eebc4c9fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20833333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    }
  ]
}