{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EfDodKJL1NH2",
        "outputId": "1cfa485d-57df-4321-da3e-b61e51c0d5ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('wordnet_ic')\n",
        "from nltk.corpus import wordnet_ic\n",
        "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import wordnet\n",
        "from itertools import chain\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkr0Kc7R2pGa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('latest_ticket_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKt5AnR-TZav"
      },
      "outputs": [],
      "source": [
        "def get_only_chars(line):\n",
        "\n",
        "    clean_line = \"\"\n",
        "\n",
        "    line = line.replace(\"â€™\", \"\")\n",
        "    line = line.replace(\"'\", \"\")\n",
        "    line = line.replace(\"-\", \" \") #replace hyphens with spaces\n",
        "    line = line.replace(\"\\t\", \" \")\n",
        "    line = line.replace(\"\\n\", \" \")\n",
        "    line = line.lower()\n",
        "\n",
        "    for char in line:\n",
        "        if char in 'qwertyuiopasdfghjklzxcvbnm ':\n",
        "            clean_line += char\n",
        "        else:\n",
        "            clean_line += ' '\n",
        "\n",
        "    clean_line = re.sub(' +',' ',clean_line) #delete extra spaces\n",
        "    if clean_line[0] == ' ':\n",
        "        clean_line = clean_line[1:]\n",
        "    return clean_line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuDZi8vXTkKf"
      },
      "outputs": [],
      "source": [
        "df['Description'] = df['Description'].apply(lambda x: get_only_chars(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB0EJWU63XKZ"
      },
      "outputs": [],
      "source": [
        "df['Tokenized']=[nltk.word_tokenize(i) for i in df['Description']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcXSr0ye3b5G"
      },
      "outputs": [],
      "source": [
        "df['pos_tagged'] = [nltk.pos_tag(i) for i in df['Tokenized']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k8OsXCby3iwn"
      },
      "outputs": [],
      "source": [
        "df['NN_tagged'] = df['pos_tagged'].apply(lambda item:[w for w,t in item if t=='NN'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "insbKP9b3nCp"
      },
      "outputs": [],
      "source": [
        "df['NN_Description'] = df.NN_tagged.map(lambda x: ' '.join(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZX8Uiht6ULEq"
      },
      "outputs": [],
      "source": [
        "stop_words = stopwords.words('english')\n",
        "custom_stop_words = ['hi', 'since', 'please', 'best', 'regards', 'thank', 'thanks', 'hello', 'sent', 'great', 'dear', 'help', 'kind']\n",
        "time_words = ['january', 'february', 'march', 'april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december',\n",
        "              'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'sunday', 'today' , 'yesterday', 'tomorrow',\n",
        "              'hour', 'hours', 'time', 'times', 'timelines', 'date', 'day', 'days', 'am', 'pm', 'morning', 'noon', 'afternoon', 'evening',\n",
        "              'night', 'winter', 'summer', 'rain', 'cold']\n",
        "\n",
        "def remove_stop_words(text):\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(stop_words) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "    return text\n",
        "\n",
        "def remove_custom_words(text):\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(custom_stop_words) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "    return text\n",
        "\n",
        "def remove_time_words(text):\n",
        "    pattern = re.compile(r'\\b(' + r'|'.join(time_words) + r')\\b\\s*')\n",
        "    text = pattern.sub('', text)\n",
        "    return text\n",
        "\n",
        "df['NN_Description'] = df[\"NN_Description\"].map(lambda x: remove_stop_words(x))\n",
        "df['NN_Description'] = df[\"NN_Description\"].map(lambda x: remove_custom_words(x))\n",
        "df['NN_Description'] = df[\"NN_Description\"].map(lambda x: remove_time_words(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8asUEwKY2sK_"
      },
      "outputs": [],
      "source": [
        "LE = LabelEncoder()\n",
        "df['label'] = LE.fit_transform(df['Category'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KKh_mRJsBy9"
      },
      "outputs": [],
      "source": [
        "def Get_Label_Synonyms(label):\n",
        "  result=label.split(\" \")\n",
        "  wordcount=len(result)\n",
        "  synlist = []\n",
        "  if(wordcount == 1):\n",
        "    if(wordnet.synsets(label)):\n",
        "      synonyms = wordnet.synsets(label)\n",
        "      synlist = list(set(chain.from_iterable([word.lemma_names() for word in synonyms])))\n",
        "  else:\n",
        "    for j in range(0,wordcount):\n",
        "      label_j = []\n",
        "      if(wordnet.synsets(result[j])):\n",
        "        synonyms = wordnet.synsets(result[j])\n",
        "        label_j = list(set(chain.from_iterable([word.lemma_names() for word in synonyms])))\n",
        "        synlist = synlist + label_j\n",
        "  synlist = [item.replace(\"_\",\" \") for item in synlist]\n",
        "  return(synlist)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VShSKA_3qgea"
      },
      "outputs": [],
      "source": [
        "labelList = df['Category'].unique().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJxJxwC0ymvK"
      },
      "outputs": [],
      "source": [
        "def getSynonymKeywords(labelList):\n",
        "  length=len(labelList)\n",
        "  topics=[]\n",
        "  for i in range(0,length):\n",
        "    label_i = Get_Label_Synonyms(labelList[i])\n",
        "    t = ' '.join(str(x) for x in label_i)\n",
        "    u = ' '.join(set(t.split()))\n",
        "    topics.append(u)\n",
        "  return(topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUIYYPuoNMDN"
      },
      "outputs": [],
      "source": [
        "def getKeywords(labelList,keywordsType):\n",
        "  topics=[]\n",
        "  if(keywordsType=='synonym'):\n",
        "    topics=getSynonymKeywords(labelList)\n",
        "    return(topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7bUg-C8q69A",
        "outputId": "c9fbecbf-c067-46c8-eaa0-8ce9e0b4241a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['programme practical covering application program applications diligence coating lotion', 'database', 'meshing web net mesh network meshwork electronic', 'alimony exploiter user substance drug criminal sustenance sustentation maintenance care abuser sustainment upkeep', 'certificate department measure system surety protection measures security']\n"
          ]
        }
      ],
      "source": [
        "topics = getKeywords(labelList,'synonym')\n",
        "print(topics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoBJD13Rku67"
      },
      "outputs": [],
      "source": [
        "# Generate samples that contains K samples of each class\n",
        "def gen_sample(df, sample_size, num_classes):\n",
        "\n",
        "    df_1 = df[(df[\"label\"]<num_classes + 1)].reset_index().drop([\"index\"], axis=1).reset_index().drop([\"index\"], axis=1)\n",
        "    train = df_1[df_1[\"label\"] == np.unique(df_1['label'])[0]].sample(sample_size)\n",
        "\n",
        "    train_index = train.index.tolist()\n",
        "\n",
        "    for i in range(1,num_classes):\n",
        "        train_2 = df_1[df_1[\"label\"] == np.unique(df_1['label'])[i]].sample(sample_size)\n",
        "        train = pd.concat([train, train_2], axis=0)\n",
        "        train_index.extend(train_2.index.tolist())\n",
        "\n",
        "    test = df_1[~df_1.index.isin(train_index)]\n",
        "\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saMjRxel4BqY"
      },
      "outputs": [],
      "source": [
        "def WordnetShortestPath_labelscore(a,topics):\n",
        "  lowest_netavg=100\n",
        "  lowest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.shortest_path_distance(syn2))\n",
        "            sum=sum+syn1.shortest_path_distance(syn2)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg<lowest_netavg):\n",
        "      lowest_netavg=netavg\n",
        "      lowest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return lowest_label,lowest_netavg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5u9bQVhqVQ4"
      },
      "outputs": [],
      "source": [
        "def WordnetLeacock_labelscore(a,topics):\n",
        "  lowest_netavg=100\n",
        "  lowest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.shortest_path_distance(syn2))\n",
        "            sum=sum+syn1.lch_similarity(syn2)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg<lowest_netavg):\n",
        "      lowest_netavg=netavg\n",
        "      lowest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return lowest_label,lowest_netavg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXfGRH2LnkxQ"
      },
      "outputs": [],
      "source": [
        "def WordnetRES_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            sum=sum+syn1.res_similarity(syn2, brown_ic)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5V_Qpb68thZn"
      },
      "outputs": [],
      "source": [
        "def WordnetLIN_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.wup_similarity(syn2))\n",
        "            sum=sum+syn1.lin_similarity(syn2, brown_ic)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7EpXseYtMCT"
      },
      "outputs": [],
      "source": [
        "def WordnetJCN_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.wup_similarity(syn2))\n",
        "            sum=sum+syn1.jcn_similarity(syn2, brown_ic)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LihS7434Nqgl"
      },
      "outputs": [],
      "source": [
        "def WordnetWUP_labelscore(a,topics):\n",
        "  highest_netavg=0\n",
        "  highest_label=0\n",
        "  label=-1\n",
        "  #print(a)\n",
        "  words = nltk.word_tokenize(a)\n",
        "  for z in topics:\n",
        "    total=0\n",
        "    counter=0\n",
        "    #print(z)\n",
        "    for x in z:\n",
        "      count=0\n",
        "      avg=0\n",
        "      sum=0\n",
        "      for y in words:\n",
        "        if(wordnet.synsets(x) and wordnet.synsets(y)):\n",
        "          syn1 = wordnet.synsets(x)[0]\n",
        "          syn2 = wordnet.synsets(y)[0]\n",
        "          if(syn1.pos() == 'n' and syn2.pos() == 'n'):\n",
        "            #print(\"Shortest path between \",x,\" and \",y,\" is: \", syn1.wup_similarity(syn2))\n",
        "            sum=sum+syn1.wup_similarity(syn2)\n",
        "            count=count+1\n",
        "      if(count==0):\n",
        "        avg=0\n",
        "      else:\n",
        "        avg=sum/count\n",
        "      total=total+avg\n",
        "      counter=counter+1\n",
        "      #print(sum)\n",
        "      #print(count)\n",
        "      #print(avg)\n",
        "    if(counter==0):\n",
        "      netavg=0\n",
        "    else:\n",
        "      netavg=total/counter\n",
        "    #print(counter)\n",
        "    #print(\"Total score for \",z,\" is: \",total)\n",
        "    #print(\"Net average for \",z,\" is: \",netavg)\n",
        "    label=label+1\n",
        "    if(netavg>highest_netavg):\n",
        "      highest_netavg=netavg\n",
        "      highest_label=label\n",
        "      #print(\"label: \",label,\" has shortest path value: \",lowest_netavg)\n",
        "  return highest_label,highest_netavg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQqGTWxPspyp"
      },
      "outputs": [],
      "source": [
        "def ZeroShotWordnetModel(text, labelList, keywordstobeGenerated='Yes', keywordsList=[], keywordsType='synonym', posfilterType='NN', measureType='ShortestPath'):\n",
        "  if not text and not labelList:\n",
        "    print(\"Input Text and List of Label/Category names required\")\n",
        "    return()\n",
        "  else:\n",
        "    if(keywordstobeGenerated=='Yes'):\n",
        "      topics = getKeywords(labelList,keywordsType)\n",
        "    else:\n",
        "      topics = labelList\n",
        "    if(posfilterType=='NN'):\n",
        "      if(measureType=='WuPalmer'):\n",
        "        return(WordnetWUP_labelscore(text,topics))\n",
        "      if(measureType=='Resnik'):\n",
        "        return(WordnetRES_labelscore(text,topics))\n",
        "      if(measureType=='JCN'):\n",
        "        return(WordnetJCN_labelscore(text,topics))\n",
        "      if(measureType=='Lin'):\n",
        "        return(WordnetLIN_labelscore(text,topics))\n",
        "      if(measureType=='Leacock'):\n",
        "        return(WordnetLeacock_labelscore(text,topics))\n",
        "      if(measureType=='ShortestPath'):\n",
        "        return(WordnetShortestPath_labelscore(text,topics))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmmAqSOak0vC"
      },
      "outputs": [],
      "source": [
        "data, rest = gen_sample(df, 100, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHPJNWfW3QmP"
      },
      "outputs": [],
      "source": [
        "df[['Leacock_Label', 'Leacock_Value']] = df['Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,measureType='Leacock')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2Jn-Oew-PkN"
      },
      "outputs": [],
      "source": [
        "df[\"Leacock_Label\"] = df[\"Leacock_Label\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osbu7UnG-VGc",
        "outputId": "5bfe2a96-5652-43d4-e9d5-a6a0d92ff721"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.19866666666666666"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(df['label'], df['Leacock_Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ik4m-QJY7NRj"
      },
      "outputs": [],
      "source": [
        "df[['Lin_Label', 'Lin_Value']] = df['Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,measureType='Lin')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikVFO_l0_RtT"
      },
      "outputs": [],
      "source": [
        "df[\"Lin_Label\"] = df[\"Lin_Label\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwv0Zb-1_Uzp",
        "outputId": "793a1c88-45fb-4523-c155-7f2d44f7d633"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.20966666666666667"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(df['label'], df['Lin_Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WGPi0GI7ntD"
      },
      "outputs": [],
      "source": [
        "df[['JCN_Label', 'JCN_Value']] = df['Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,measureType='JCN')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oJyn65aj_aeB"
      },
      "outputs": [],
      "source": [
        "df[\"JCN_Label\"] = df[\"JCN_Label\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCSMW2fk_bMv",
        "outputId": "6c3d4f19-7ee4-45f0-ac62-03f93da70481"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.19766666666666666"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(df['label'], df['JCN_Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ctvZe8i7xKv"
      },
      "outputs": [],
      "source": [
        "df[['Resnik_Label', 'Resnik_Value']] = df['Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,measureType='Resnik')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5sa3gcOy_iB7"
      },
      "outputs": [],
      "source": [
        "df[\"Resnik_Label\"] = df[\"Resnik_Label\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nL_ayQq8ACa3",
        "outputId": "47f6ad8c-1ab4-4e31-b016-c505eff986f0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.22966666666666666"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(df['label'], df['Resnik_Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iZZpwWOA8TUS"
      },
      "outputs": [],
      "source": [
        "df[['WUP_Label', 'WUP_Value']] = df['Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList,measureType='WuPalmer')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gQ3lWNGZAA_S"
      },
      "outputs": [],
      "source": [
        "df[\"WUP_Label\"] = df[\"WUP_Label\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tcWFehVpAFSq",
        "outputId": "016a623e-99b8-4658-eeaa-1b7fbdaf6a40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.206"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "accuracy_score(df['label'], df['WUP_Label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbC17Tx3HKpM"
      },
      "outputs": [],
      "source": [
        "df[['WSP_Label', 'WSP_Value']] = df['Description'].apply(lambda x: pd.Series(ZeroShotWordnetModel(x, labelList)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J8jXVYc_sZq"
      },
      "outputs": [],
      "source": [
        "df[\"WSP_Label\"] = df[\"WSP_Label\"].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V_5rDwzNbaQ"
      },
      "outputs": [],
      "source": [
        "accuracy_score(df['label'], df['WSP_Label'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}